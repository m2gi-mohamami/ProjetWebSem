{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHs7Vs8suwLu",
        "outputId": "aa669513-1b67-41b2-a82b-9a7ea77dba96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Taille totale du fichier (Go): 0.875\n",
            "Fichier unique généré avec 10000 lignes dans : output_files1_csv/part_1.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import gzip\n",
        "\n",
        "# Paramètres\n",
        "input_file = \"en.openfoodfacts.org.products.csv.gz\"  # Fichier CSV.gz source\n",
        "output_dir = \"output_files1_csv\"  # Dossier pour les fichiers de sortie\n",
        "output_file = os.path.join(output_dir, \"part_1.csv\")  # Fichier de sortie unique\n",
        "max_lines = 10000  # Nombre de lignes maximum dans le fichier, y compris les en-têtes\n",
        "\n",
        "# Créer le dossier de sortie s'il n'existe pas\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Taille totale du fichier\n",
        "total_size_bytes = os.path.getsize(input_file)\n",
        "print(\"Taille totale du fichier (Go):\", total_size_bytes / (1024 ** 3))\n",
        "\n",
        "# Lire et écrire les données dans un fichier CSV unique\n",
        "with gzip.open(input_file, \"rt\", encoding=\"utf-8\") as infile, open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
        "    # Lire les en-têtes\n",
        "    header = infile.readline()\n",
        "    outfile.write(header)\n",
        "\n",
        "    # Initialisation\n",
        "    line_count = 0\n",
        "\n",
        "    for line in infile:\n",
        "        # Écrire la ligne dans le fichier de sortie\n",
        "        outfile.write(line)\n",
        "        line_count += 1\n",
        "\n",
        "        # Arrêter après avoir écrit le nombre maximum de lignes\n",
        "        if line_count >= max_lines:\n",
        "            break\n",
        "\n",
        "print(f\"Fichier unique généré avec {line_count} lignes dans : {output_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chargement et comprehension du fichier csv\n"
      ],
      "metadata": {
        "id": "QxuTwIqazLRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "# Chemin du fichier CSV\n",
        "file_path = 'output_files1_csv/part_1.csv'\n",
        "\n",
        "# Vérifier si le fichier existe\n",
        "if not os.path.exists(file_path):\n",
        "    print(f\"Le fichier {file_path} n'existe pas.\")\n",
        "    exit()\n",
        "\n",
        "# Chemin du fichier texte contenant les colonnes principales\n",
        "columns_file_path = 'important_columns.txt'\n",
        "\n",
        "# Charger les colonnes principales à partir du fichier texte\n",
        "try:\n",
        "    with open(columns_file_path, 'r') as file:\n",
        "        selected_columns = [line.strip() for line in file.readlines() if line.strip()]\n",
        "except Exception as e:\n",
        "    print(f\"Erreur lors du chargement des colonnes depuis le fichier texte : {e}\")\n",
        "    exit()\n",
        "\n",
        "# Charger le fichier CSV\n",
        "try:\n",
        "    df = pd.read_csv(file_path, delimiter='\\t', low_memory=False)\n",
        "except Exception as e:\n",
        "    print(f\"Erreur lors du chargement du fichier : {e}\")\n",
        "    exit()\n",
        "\n",
        "# Étape 1 : Garder uniquement les colonnes nécessaires\n",
        "if all(col in df.columns for col in selected_columns):\n",
        "    df = df[selected_columns]\n",
        "else:\n",
        "    print(\"Les colonnes sélectionnées ne sont pas toutes présentes dans le fichier.\")\n",
        "    exit()\n",
        "\n",
        "# Étape 2 : Supprimer les doublons\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "\n",
        "# Sauvegarder le fichier nettoyé\n",
        "cleaned_file_path = 'cleaned_file.csv'\n",
        "df.to_csv(cleaned_file_path, index=False, encoding='utf-8')\n",
        "\n",
        "# Aperçu des premières lignes\n",
        "print(f\"Fichier nettoyé sauvegardé dans : {cleaned_file_path}\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8KDCfb9zGkS",
        "outputId": "22a75259-8d63-4755-b1c5-eb378e71d1eb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Les colonnes sélectionnées ne sont pas toutes présentes dans le fichier.\n",
            "Fichier nettoyé sauvegardé dans : cleaned_file.csv\n",
            "   code                                                url     creator  \\\n",
            "0    54  http://world-en.openfoodfacts.org/product/0000...     kiliweb   \n",
            "1    63  http://world-en.openfoodfacts.org/product/0000...     kiliweb   \n",
            "2     1  http://world-en.openfoodfacts.org/product/0000...         inf   \n",
            "3     2  http://world-en.openfoodfacts.org/product/0000...     kiliweb   \n",
            "4     3  http://world-en.openfoodfacts.org/product/0000...  prepperapp   \n",
            "\n",
            "    created_t      created_datetime  last_modified_t last_modified_datetime  \\\n",
            "0  1582569031  2020-02-24T18:30:31Z       1733085204   2024-12-01T20:33:24Z   \n",
            "1  1673620307  2023-01-13T14:31:47Z       1732913331   2024-11-29T20:48:51Z   \n",
            "2  1634745456  2021-10-20T15:57:36Z       1734203090   2024-12-14T19:04:50Z   \n",
            "3  1722606455  2024-08-02T13:47:35Z       1734216390   2024-12-14T22:46:30Z   \n",
            "4  1716818343  2024-05-27T13:59:03Z       1732748475   2024-11-27T23:01:15Z   \n",
            "\n",
            "   last_modified_by  last_updated_t last_updated_datetime  ...  \\\n",
            "0               NaN      1733085204  2024-12-01T20:33:24Z  ...   \n",
            "1  insectproductadd      1732913331  2024-11-29T20:48:51Z  ...   \n",
            "2      smoothie-app      1734366039  2024-12-16T16:20:39Z  ...   \n",
            "3      smoothie-app      1734216390  2024-12-14T22:46:30Z  ...   \n",
            "4         foodvisor      1734373452  2024-12-16T18:24:12Z  ...   \n",
            "\n",
            "  glycemic-index_100g water-hardness_100g choline_100g phylloquinone_100g  \\\n",
            "0                 NaN                 NaN          NaN                NaN   \n",
            "1                 NaN                 NaN          NaN                NaN   \n",
            "2                 NaN                 NaN          NaN                NaN   \n",
            "3                 NaN                 NaN          NaN                NaN   \n",
            "4                 NaN                 NaN          NaN                NaN   \n",
            "\n",
            "  beta-glucan_100g inositol_100g carnitine_100g sulphate_100g nitrate_100g  \\\n",
            "0              NaN           NaN            NaN           NaN          NaN   \n",
            "1              NaN           NaN            NaN           NaN          NaN   \n",
            "2              NaN           NaN            NaN           NaN          NaN   \n",
            "3              NaN           NaN            NaN           NaN          NaN   \n",
            "4              NaN           NaN            NaN           NaN          NaN   \n",
            "\n",
            "  acidity_100g  \n",
            "0          NaN  \n",
            "1          NaN  \n",
            "2          NaN  \n",
            "3          NaN  \n",
            "4          NaN  \n",
            "\n",
            "[5 rows x 206 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install rdflib\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PystJcH_1Nq3",
        "outputId": "8dc72116-b6bf-489e-def9-db6eabd0789d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdflib\n",
            "  Downloading rdflib-7.1.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting isodate<1.0.0,>=0.7.2 (from rdflib)\n",
            "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from rdflib) (3.2.0)\n",
            "Downloading rdflib-7.1.1-py3-none-any.whl (562 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m562.4/562.4 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: isodate, rdflib\n",
            "Successfully installed isodate-0.7.2 rdflib-7.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import urllib.parse\n",
        "from rdflib import Graph, Namespace, URIRef, Literal, RDF, RDFS\n",
        "from rdflib.namespace import OWL, XSD, FOAF, DC\n",
        "\n",
        "# Chemins des fichiers\n",
        "csv_file_path = \"cleaned_file.csv\"  # Remplacez par votre fichier CSV\n",
        "txt_file_path = \"important_columns.txt\"  # Fichier texte contenant les colonnes importantes\n",
        "rdf_file_path = \"output_ontology.owl\"  # Fichier de sortie OWL\n",
        "\n",
        "# Espaces de noms pour l'ontologie\n",
        "SCHEMA = Namespace(\"http://schema.org/\")\n",
        "DBO = Namespace(\"http://dbpedia.org/ontology/\")\n",
        "EX = Namespace(\"http://produitsalimentaires.fr/\")\n",
        "FOAF = Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
        "XSD = Namespace(\"http://www.w3.org/2001/XMLSchema#\")\n",
        "\n",
        "\n",
        "\n",
        "# Charger la liste des colonnes importantes depuis le fichier texte\n",
        "try:\n",
        "    with open(txt_file_path, \"r\") as file:\n",
        "        important_columns = [line.strip() for line in file.readlines() if line.strip()]\n",
        "except Exception as e:\n",
        "    print(f\"Erreur lors du chargement des colonnes : {e}\")\n",
        "    exit()\n",
        "\n",
        "# Liaison des namespaces au graphe\n",
        "g = Graph()\n",
        "g.bind(\"schema\", SCHEMA)\n",
        "g.bind(\"dbo\", DBO)\n",
        "g.bind(\"foaf\", FOAF)\n",
        "g.bind(\"dc\", DC)\n",
        "g.bind(\"ex\", EX)\n",
        "g.bind(\"owl\", OWL)\n",
        "\n",
        "# Définition de l'ontologie\n",
        "g.add((URIRef(\"http://produitsalimentaires.fr/foaf-ontology\"), RDF.type, OWL.Ontology))\n",
        "g.add((URIRef(\"http://produitsalimentaires.fr/foaf-ontology\"), RDFS.comment, Literal(\n",
        "    \"Une ontologie\"\n",
        ")))\n",
        "\n",
        "# Définir les classes principales et sous-classes\n",
        "g.add((EX.Product, RDF.type, OWL.Class))\n",
        "g.add((DBO.Product, RDF.type, OWL.Class))\n",
        "\n",
        "g.add((EX.Brand, RDF.type, OWL.Class))\n",
        "g.add((DBO.Brand, RDF.type, OWL.Class))\n",
        "\n",
        "g.add((EX.Category, RDF.type, OWL.Class))\n",
        "g.add((DBO.Category, RDF.type, OWL.Class))\n",
        "\n",
        "g.add((EX.Nutrient, RDF.type, OWL.Class))\n",
        "g.add((EX.Allergen, RDF.type, OWL.Class))\n",
        "#g.add((EX.Origin, RDF.type, OWL.Class))\n",
        "#g.add((EX.Addiction, RDF.type, OWL.Class))\n",
        "g.add((EX.Country, RDF.type, OWL.Class))\n",
        "g.add((DBO.Country, RDF.type, OWL.Class))\n",
        "\n",
        "g.add((EX.Creator, RDF.type, OWL.Class))\n",
        "g.add((EX.ManufacturingPlace, RDF.type, OWL.Class))\n",
        "g.add((EX.Ingredient, RDF.type, OWL.Class))\n",
        "g.add((EX.PurchasePlace, RDF.type, OWL.Class))\n",
        "g.add((EX.City, RDF.type, OWL.Class))\n",
        "#g.add((EX.Ecoscore, RDF.type, OWL.Class))\n",
        "\n",
        "# Sous-classes potentielles (exemples)\n",
        "#g.add((EX.EcoscoreScore, RDF.type, OWL.Class))\n",
        "#g.add((EX.EcoscoreGrade, RDF.type, OWL.Class))\n",
        "\n",
        "# Définir les propriétés et leur domaine et plage\n",
        "g.add((EX.hasBrand, RDF.type, OWL.ObjectProperty))\n",
        "g.add((EX.hasBrand, RDFS.domain, EX.Product))\n",
        "g.add((EX.hasBrand, RDFS.range, EX.Brand))\n",
        "\n",
        "g.add((EX.hasCategory, RDF.type, OWL.ObjectProperty))\n",
        "g.add((EX.hasCategory, RDFS.domain, EX.Product))\n",
        "g.add((EX.hasCategory, RDFS.range, EX.Category))\n",
        "\n",
        "#g.add((EX.hasOrigin, RDF.type, OWL.ObjectProperty))\n",
        "#g.add((EX.hasOrigin, RDFS.domain, EX.Product))\n",
        "#g.add((EX.hasOrigin, RDFS.range, EX.Origin))\n",
        "\n",
        "g.add((EX.containsAllergen, RDF.type, OWL.ObjectProperty))\n",
        "g.add((EX.containsAllergen, RDFS.domain, EX.Product))\n",
        "g.add((EX.containsAllergen, RDFS.range, EX.Allergen))\n",
        "\n",
        "g.add((EX.hasNutrient, RDF.type, OWL.ObjectProperty))\n",
        "g.add((EX.hasNutrient, RDFS.domain, EX.Product))\n",
        "g.add((EX.hasNutrient, RDFS.range, EX.Nutrient))\n",
        "\n",
        "#g.add((EX.hasAddiction, RDF.type, OWL.ObjectProperty))\n",
        "#g.add((EX.hasAddiction, RDFS.domain, EX.Product))\n",
        "#g.add((EX.hasAddiction, RDFS.range, EX.Addiction))\n",
        "\n",
        "g.add((EX.availableInCountry, RDF.type, OWL.ObjectProperty))\n",
        "g.add((EX.availableInCountry, RDFS.domain, EX.Product))\n",
        "g.add((EX.availableInCountry, RDFS.range, DBO.Country))\n",
        "\n",
        "# Nouvelles propriétés pour les entités supplémentaires\n",
        "g.add((EX.hasCreator, RDF.type, OWL.ObjectProperty))\n",
        "g.add((EX.hasCreator, RDFS.domain, EX.Product))\n",
        "g.add((EX.hasCreator, RDFS.range, EX.Creator))\n",
        "\n",
        "g.add((EX.hasManufacturingPlace, RDF.type, OWL.ObjectProperty))\n",
        "g.add((EX.hasManufacturingPlace, RDFS.domain, EX.Product))\n",
        "g.add((EX.hasManufacturingPlace, RDFS.range, EX.ManufacturingPlace))\n",
        "\n",
        "g.add((EX.hasIngredient, RDF.type, OWL.ObjectProperty))\n",
        "g.add((EX.hasIngredient, RDFS.domain, EX.Product))\n",
        "g.add((EX.hasIngredient, RDFS.range, EX.Ingredient))\n",
        "\n",
        "\n",
        "\n",
        "g.add((EX.hasIngredientsText, RDF.type, OWL.DatatypeProperty))\n",
        "g.add((EX.hasIngredientsText, RDFS.domain, EX.Product))\n",
        "g.add((EX.hasIngredientsText, RDFS.range, XSD.string))\n",
        "\n",
        "# Ajouter les propriétés pour quantity et serving_size\n",
        "g.add((EX.hasQuantity, RDF.type, OWL.DatatypeProperty))\n",
        "g.add((EX.hasQuantity, RDFS.domain, EX.Product))\n",
        "g.add((EX.hasQuantity, RDFS.range, XSD.string))\n",
        "\n",
        "#g.add((EX.hasServingSize, RDF.type, OWL.DatatypeProperty))\n",
        "#g.add((EX.hasServingSize, RDFS.domain, EX.Product))\n",
        "#g.add((EX.hasServingSize, RDFS.range, XSD.string))\n",
        "\n",
        "\n",
        "g.add((EX.hasPurchasePlace, RDF.type, OWL.ObjectProperty))\n",
        "g.add((EX.hasPurchasePlace, RDFS.domain, EX.Product))\n",
        "g.add((EX.hasPurchasePlace, RDFS.range, EX.PurchasePlace))\n",
        "\n",
        "g.add((EX.hasCity, RDF.type, OWL.ObjectProperty))\n",
        "g.add((EX.hasCity, RDFS.domain, EX.Product))\n",
        "g.add((EX.hasCity, RDFS.range, EX.City))\n",
        "\n",
        "\"\"\"g.add((EX.hasEcoscore, RDF.type, OWL.ObjectProperty))\n",
        "g.add((EX.hasEcoscore, RDFS.domain, EX.Product))\n",
        "g.add((EX.hasEcoscore, RDFS.range, EX.Ecoscore))\n",
        "\n",
        "g.add((EX.hasEcoscoreScore, RDF.type, OWL.ObjectProperty))\n",
        "g.add((EX.hasEcoscoreScore, RDFS.domain, EX.Ecoscore))\n",
        "g.add((EX.hasEcoscoreScore, RDFS.range, EX.EcoscoreScore))\n",
        "\n",
        "#g.add((EX.hasEcoscoreGrade, RDF.type, OWL.ObjectProperty))\n",
        "g.add((EX.hasEcoscoreGrade, RDFS.domain, EX.Ecoscore))\n",
        "g.add((EX.hasEcoscoreGrade, RDFS.range, EX.EcoscoreGrade))\"\"\"\n",
        "\n",
        "# Fonction pour encoder les URI de manière sécurisée\n",
        "def encode_uri(uri):\n",
        "    return urllib.parse.quote(uri, safe=\":/#?&=~\")\n",
        "\n",
        "# Lire le fichier CSV et ajouter des instances\n",
        "try:\n",
        "    with open(csv_file_path, newline='', encoding='utf-8') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        for row in reader:\n",
        "\n",
        "            if all(col in row for col in important_columns):\n",
        "                product_uri = URIRef(encode_uri(f\"http://produitsalimentaires.fr/product/{row['code']}\"))\n",
        "\n",
        "                # Ajouter le type de produit\n",
        "\n",
        "                g.add((product_uri, RDF.type, DBO.Product))\n",
        "                g.add((product_uri, SCHEMA.name, Literal(row[\"product_name\"])))\n",
        "                g.add((product_uri, SCHEMA.url, URIRef(row[\"url\"])))\n",
        "\n",
        "\n",
        "\n",
        "                # Ajouter la marque\n",
        "                if row[\"brands\"]:\n",
        "                    brand_uri = URIRef(encode_uri(f\"http://produitsalimentaires.fr/brand/{row['brands']}\"))\n",
        "                    g.add((product_uri, EX.hasBrand, brand_uri))\n",
        "                    g.add((brand_uri, RDF.type, DBO.Brand))\n",
        "                    g.add((brand_uri, FOAF.name, Literal(row[\"brands\"], lang=\"en\")))\n",
        "\n",
        "                # Ajouter des catégories basées sur le champ 'categories'\n",
        "                if row[\"categories\"]:\n",
        "                    categories = row[\"categories\"].split(',')  # Séparer les catégories par des virgules\n",
        "                    for category in categories:\n",
        "                        category = category.strip()\n",
        "                        if category:  # Vérifier que la catégorie n'est pas vide\n",
        "                            category_uri = URIRef(encode_uri(f\"http://produitsalimentaires.fr/category/{category}\"))\n",
        "\n",
        "                            g.add((product_uri, EX.hasCategory, category_uri))\n",
        "                            g.add((category_uri, RDF.type, EX.Category))\n",
        "                            g.add((category_uri, FOAF.name, Literal(category, lang=\"en\")))\n",
        "\n",
        "                # Ajouter des relations d'origine basées sur le champ 'origins'\n",
        "                #if row[\"origins\"]:\n",
        "                    #origins = row[\"origins\"].split(',')  # Séparer les valeurs par des virgules\n",
        "                    #for origin in origins:\n",
        "                       # origin = origin.strip()\n",
        "                        #if origin:  # Vérifier que l'origine n'est pas vide\n",
        "                            # Créer une instance de l'origine\n",
        "                           # origin_uri = URIRef(encode_uri(f\"http://produitsalimentaires.fr/product/origin/{origin}\"))\n",
        "\n",
        "                            #g.add((product_uri, EX.hasOrigin, origin_uri))\n",
        "                            #g.add((origin_uri, RDF.type, EX.Origin))\n",
        "                            #g.add((origin_uri, SCHEMA.name, Literal(origin)))\n",
        "\n",
        "                if row[\"allergens\"]:\n",
        "                    allergens = row[\"allergens\"].split(',')  # Séparer les valeurs par des virgules\n",
        "                    for allergen in allergens:\n",
        "                        allergen = allergen.strip()\n",
        "                        if allergen:  # Vérifier que l'origine n'est pas vide\n",
        "                            # Créer une instance de l'origine\n",
        "                            allergen_uri = URIRef(encode_uri(f\"http://produitsalimentaires.fr/allergens/{allergen}\"))\n",
        "\n",
        "                            g.add((product_uri, EX.containsAllergen, allergen_uri))\n",
        "                            g.add((allergen_uri, RDF.type, EX.Allergen))\n",
        "                            g.add((allergen_uri, FOAF.name, Literal(allergen, lang=\"en\")))\n",
        "\n",
        "                # Ajouter des addictions\n",
        "                #if row[\"additives\"]:\n",
        "                    #addiction_uri = URIRef(encode_uri(f\"http://produitsalimentaires.fr/product/addiction/{row['additives']}\"))\n",
        "                    #print(f\"Adding addictives URI: {addiction_uri}\")\n",
        "                    #g.add((product_uri, EX.hasAddiction, addiction_uri))\n",
        "                    #g.add((addiction_uri, RDF.type, EX.Addiction))\n",
        "                    #g.add((addiction_uri, SCHEMA.name, Literal(row[\"additives\"])))\n",
        "\n",
        "                # Ajouter des pays de disponibilité\n",
        "                #if row[\"countries\"]:\n",
        "                   # country_uri = URIRef(encode_uri(f\"http://produitsalimentaires.fr/country/{row['countries']}\"))\n",
        "                   # g.add((product_uri, EX.availableInCountry, country_uri))\n",
        "                    #g.add((country_uri, RDF.type, DBO.Country))\n",
        "                    #g.add((country_uri, SCHEMA.name, Literal(row[\"countries\"])))\n",
        "\n",
        "                if row[\"countries\"]:\n",
        "                    countries = row[\"countries\"].split(',')  # Séparer les catégories par des virgules\n",
        "                    for category in countries:\n",
        "                        category = category.strip()\n",
        "                        if category:  # Vérifier que la catégorie n'est pas vide\n",
        "                            country_uri = URIRef(encode_uri(f\"http://produitsalimentaires.fr/countries/{category}\"))\n",
        "                            g.add((product_uri, EX.availableInCountry, country_uri))\n",
        "                            g.add((category_uri, RDF.type, DBO.Country))\n",
        "                            g.add((category_uri, FOAF.name, Literal(category, lang=\"en\")))\n",
        "\n",
        "                # Ajouter des nutriments\n",
        "                nutrients = [\n",
        "                    (\"energy_100g\", \"Energy\"),\n",
        "                    (\"fat_100g\", \"Fat\"),\n",
        "                    (\"saturated-fat_100g\", \"Saturated Fat\"),\n",
        "                    (\"carbohydrates_100g\", \"Carbohydrates\"),\n",
        "                    (\"sugars_100g\", \"Sugars\"),\n",
        "                    (\"fiber_100g\", \"Fiber\"),\n",
        "                    (\"proteins_100g\", \"Proteins\"),\n",
        "                    (\"salt_100g\", \"Salt\"),\n",
        "                    (\"carbon-footprint_100g\", \"carbon footprint\")\n",
        "                ]\n",
        "                for nutrient, label in nutrients:\n",
        "                    if row[nutrient]:\n",
        "                        nutrient_uri = URIRef(encode_uri(f\"http://produitsalimentaires.fr/nutrient/{label}\"))\n",
        "                        g.add((product_uri, EX.hasNutrient, nutrient_uri))\n",
        "                        g.add((nutrient_uri, RDF.type, EX.Nutrient))\n",
        "                        g.add((nutrient_uri, SCHEMA.name, Literal(label)))\n",
        "                        g.add((nutrient_uri, SCHEMA.value, Literal(row[nutrient], datatype=XSD.float)))\n",
        "\n",
        "                # Ajouter des ingrédients\n",
        "                if row[\"ingredients_text\"]:\n",
        "                    g.add((product_uri, EX.hasIngredientsText, Literal(row[\"ingredients_text\"])))\n",
        "\n",
        "                # Ajouter les créateurs\n",
        "                if row[\"creator\"]:\n",
        "                    creator_uri = URIRef(encode_uri(f\"http://produitsalimentaires.fr/creator/{row['creator']}\"))\n",
        "                    g.add((product_uri, EX.hasCreator, creator_uri))\n",
        "                    g.add((creator_uri, RDF.type, EX.Creator))\n",
        "                    g.add((creator_uri, FOAF.name, Literal(row[\"creator\"], lang=\"en\")))\n",
        "\n",
        "                # Ajouter les lieux de fabrication\n",
        "                if row[\"manufacturing_places\"]:\n",
        "                    manufacturing = row[\"manufacturing_places\"].split(',')  # Séparer les catégories par des virgules\n",
        "                    for manu in manufacturing:\n",
        "                        cmanu = manu.strip()\n",
        "                        if category:  # Vérifier que la catégorie n'est pas vide\n",
        "\n",
        "                            #Ajouter des instances\n",
        "                            manufacturing_place_uri = URIRef(encode_uri(f\"http://produitsalimentaires.fr/manufacturing_place/{manu}\"))\n",
        "\n",
        "                            # Définir les relations et propriétés\n",
        "                            g.add((product_uri, EX.hasManufacturingPlace, manufacturing_place_uri))\n",
        "                            g.add((manufacturing_place_uri, RDF.type, EX.ManufacturingPlace))\n",
        "                            g.add((manufacturing_place_uri, FOAF.name, Literal(manu, lang=\"en\")))\n",
        "\n",
        "\n",
        "\n",
        "                # Ajouter les lieux d'achat\n",
        "                if row[\"purchase_places\"]:\n",
        "                    purchase_place_uri = URIRef(encode_uri(f\"http://produitsalimentaires.fr/purchase_place/{row['purchase_places']}\"))\n",
        "                    g.add((product_uri, EX.hasPurchasePlace, purchase_place_uri))\n",
        "                    g.add((purchase_place_uri, RDF.type, EX.PurchasePlace))\n",
        "                    g.add((purchase_place_uri, SCHEMA.name, Literal(row[\"purchase_places\"])))\n",
        "\n",
        "                # Ajouter les villes\n",
        "                if row[\"cities\"]:\n",
        "                    city_uri = URIRef(encode_uri(f\"http://produitsalimentaires.fr/city/{row['cities']}\"))\n",
        "                    g.add((product_uri, EX.hasCity, city_uri))\n",
        "                    g.add((city_uri, RDF.type, EX.City))\n",
        "                    g.add((city_uri, FOAF.name, Literal(row[\"cities\"], lang=\"en\")))\n",
        "\n",
        "                  # Ajouter la quantité (quantity)\n",
        "                if row[\"quantity\"]:\n",
        "                    g.add((product_uri, EX.hasQuantity, Literal(row[\"quantity\"])))\n",
        "\n",
        "                # Ajouter la taille de portion (serving_size)\n",
        "                #if row[\"serving_size\"]:\n",
        "                   # g.add((product_uri, EX.hasServingSize, Literal(row[\"serving_size\"])))\n",
        "\n",
        "                # Ajouter l'écorescore (score et grade)\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Erreur lors du chargement du fichier CSV : {e}\")\n",
        "    exit()\n",
        "\n",
        "# Sauvegarder le graphe OWL\n",
        "g.serialize(destination=rdf_file_path, format=\"xml\")\n",
        "print(f\"Ontologie OWL générée et sauvegardée dans : {rdf_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZ9xTj7j1CV9",
        "outputId": "938743c1-4641-4752-e596-2eb6f15dc341"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ontologie OWL générée et sauvegardée dans : output_ontology.owl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rdflib import Graph\n",
        "\n",
        "# Charger le graphe RDF depuis le fichier OWL\n",
        "file_path = \"output_ontology.owl\"\n",
        "g = Graph()\n",
        "try:\n",
        "    g.parse(file_path, format=\"xml\")  # OWL est souvent au format RDF/XML\n",
        "    print(f\"Ontologie OWL chargée depuis : {file_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Erreur lors du chargement de l'ontologie OWL : {e}\")\n",
        "    exit()\n",
        "# Sauvegarder le graphe au format JSON-LD\n",
        "jsonld_file_path = \"output_ontology.jsonld\"\n",
        "g.serialize(destination=jsonld_file_path, format=\"json-ld\", indent=4)\n",
        "print(f\"Graphe RDF converti et sauvegardé au format JSON-LD dans : {jsonld_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4pYyYCw14rQ",
        "outputId": "a04b0cbf-69c0-4ab1-9c33-c8d7923ed867"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ontologie OWL chargée depuis : output_ontology.owl\n",
            "Graphe RDF converti et sauvegardé au format JSON-LD dans : output_ontology.jsonld\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rdflib import Graph\n",
        "\n",
        "# Chemins des fichiers\n",
        "owl_file_path = \"output_ontology.owl\"  # Chemin vers votre fichier OWL\n",
        "ttl_file_path = \"output_ontology.ttl\"  # Chemin du fichier Turtle de sortie\n",
        "\n",
        "# Charger le fichier OWL\n",
        "g = Graph()\n",
        "try:\n",
        "    g.parse(owl_file_path, format=\"xml\")  # OWL est souvent au format RDF/XML\n",
        "    print(f\"Ontologie OWL chargée depuis : {owl_file_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Erreur lors du chargement de l'ontologie OWL : {e}\")\n",
        "    exit()\n",
        "\n",
        "# Sauvegarder le graphe en Turtle\n",
        "try:\n",
        "    g.serialize(destination=ttl_file_path, format=\"turtle\")\n",
        "    print(f\"Fichier Turtle RDF généré avec succès dans : {ttl_file_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Erreur lors de la sauvegarde du fichier Turtle : {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgDzlLGD1-KR",
        "outputId": "6e73e9c8-e204-4202-a49a-eaab5f4cee49"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ontologie OWL chargée depuis : output_ontology.owl\n",
            "Fichier Turtle RDF généré avec succès dans : output_ontology.ttl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rdflib import Graph\n",
        "\n",
        "# Chemin vers le fichier source (.owl)\n",
        "input_file_path = \"output_ontology.owl\"  # Remplacez par le chemin de votre fichier .owl\n",
        "\n",
        "# Chemin vers le fichier de sortie (.rdf)\n",
        "output_file_path = \"output_ontology.rdf\"  # Fichier de sortie au format RDF/XML\n",
        "\n",
        "\n",
        "# Charger le fichier OWL dans un graphe RDFLib\n",
        "g = Graph()\n",
        "try:\n",
        "    g.parse(input_file_path, format=\"xml\")  # OWL est souvent au format RDF/XML\n",
        "    print(f\"Ontologie OWL chargée depuis : {input_file_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Erreur lors du chargement de l'ontologie OWL : {e}\")\n",
        "    exit()\n",
        "\n",
        "# Sauvegarder le graphe en rdf\n",
        "try:\n",
        "    g.serialize(destination=output_file_path, format=\"xml\")\n",
        "    print(f\"Fichier RDF généré avec succès dans : {output_file_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Erreur lors de la sauvegarde du fichier RDF : {e}\")\n",
        "\n",
        "\n",
        "print(f\"Conversion terminée. Le fichier RDF a été enregistré sous : {output_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4l6f2AKN2Ii8",
        "outputId": "f0ddd561-b9b7-4303-ccd2-66b52a6bac11"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ontologie OWL chargée depuis : output_ontology.owl\n",
            "Fichier RDF généré avec succès dans : output_ontology.rdf\n",
            "Conversion terminée. Le fichier RDF a été enregistré sous : output_ontology.rdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enrichir les Données avec des Liens vers DBpedia et Wikidata"
      ],
      "metadata": {
        "id": "zDKg-EWt2OO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SPARQLWrapper\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhmoAiqN2fCE",
        "outputId": "bfb155be-0757-4721-fc26-d77778141491"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting SPARQLWrapper\n",
            "  Downloading SPARQLWrapper-2.0.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: rdflib>=6.1.1 in /usr/local/lib/python3.10/dist-packages (from SPARQLWrapper) (7.1.1)\n",
            "Requirement already satisfied: isodate<1.0.0,>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from rdflib>=6.1.1->SPARQLWrapper) (0.7.2)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from rdflib>=6.1.1->SPARQLWrapper) (3.2.0)\n",
            "Downloading SPARQLWrapper-2.0.0-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: SPARQLWrapper\n",
            "Successfully installed SPARQLWrapper-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%pip install SPARQLWrapper\n",
        "import csv\n",
        "import json\n",
        "import urllib.parse\n",
        "import requests\n",
        "from rdflib import Graph, Namespace, URIRef, Literal, RDF, RDFS\n",
        "from rdflib.namespace import OWL, XSD, FOAF, DC\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "import logging\n",
        "import os\n",
        "\n",
        "# Chemins des fichiers\n",
        "txt_file_path = \"important_columns.txt\"  # Fichier texte contenant les colonnes importantes\n",
        "rdf_file_path = \"output_ontology2.rdf\"  # Fichier de sortie OWL\n",
        "\n",
        "# Espaces de noms pour l'ontologie\n",
        "SCHEMA = Namespace(\"http://schema.org/\")\n",
        "DBO = Namespace(\"http://dbpedia.org/ontology/\")\n",
        "EX = Namespace(\"http://produitsalimentaires.fr/\")\n",
        "FOAF = Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
        "XSD = Namespace(\"http://www.w3.org/2001/XMLSchema#\")\n",
        "\n",
        "# Charger la liste des colonnes importantes depuis le fichier texte\n",
        "try:\n",
        "    with open(txt_file_path, \"r\") as file:\n",
        "        important_columns = [line.strip() for line in file.readlines() if line.strip()]\n",
        "except Exception as e:\n",
        "    print(f\"Erreur lors du chargement des colonnes : {e}\")\n",
        "    exit()\n",
        "\n",
        "# Liaison des namespaces au graphe\n",
        "g = Graph()\n",
        "g.bind(\"schema\", SCHEMA)\n",
        "g.bind(\"dbo\", DBO)\n",
        "g.bind(\"foaf\", FOAF)\n",
        "g.bind(\"dc\", DC)\n",
        "g.bind(\"ex\", EX)\n",
        "g.bind(\"owl\", OWL)\n",
        "\n",
        "\n",
        "\n",
        "# Définition de l'ontologie\n",
        "g.add((URIRef(\"http://produitsalimentaires.fr/foaf-ontology\"), RDF.type, OWL.Ontology))\n",
        "g.add((URIRef(\"http://produitsalimentaires.fr/foaf-ontology\"), RDFS.comment, Literal(\"Une ontologie\")))\n",
        "\n",
        "# Définir les classes principales et sous-classes\n",
        "g.add((EX.Product, RDF.type, OWL.Class))\n",
        "g.add((DBO.Product, RDF.type, OWL.Class))\n",
        "\n",
        "g.add((EX.Brand, RDF.type, OWL.Class))\n",
        "g.add((DBO.Brand, RDF.type, OWL.Class))\n",
        "\n",
        "g.add((EX.Category, RDF.type, OWL.Class))\n",
        "g.add((DBO.Category, RDF.type, OWL.Class))\n",
        "\n",
        "g.add((EX.Nutrient, RDF.type, OWL.Class))\n",
        "g.add((EX.Allergen, RDF.type, OWL.Class))\n",
        "\n",
        "g.add((EX.Creator, RDF.type, OWL.Class))\n",
        "g.add((EX.ManufacturingPlace, RDF.type, OWL.Class))\n",
        "g.add((EX.Ingredient, RDF.type, OWL.Class))\n",
        "g.add((EX.PurchasePlace, RDF.type, OWL.Class))\n",
        "g.add((EX.City, RDF.type, OWL.Class))\n",
        "\n",
        "\n",
        "# Définir les propriétés et leur domaine et plage\n",
        "g.add((EX.hasBrand, RDF.type, OWL.ObjectProperty))\n",
        "g.add((EX.hasBrand, RDFS.domain, EX.Product))\n",
        "g.add((EX.hasBrand, RDFS.range, EX.Brand))\n",
        "g.add((EX.Country, RDF.type, OWL.Class))\n",
        "g.add((DBO.Country, RDF.type, OWL.Class))\n",
        "\n",
        "g.add((EX.hasCategory, RDF.type, OWL.ObjectProperty))\n",
        "g.add((EX.hasCategory, RDFS.domain, EX.Product))\n",
        "g.add((EX.hasCategory, RDFS.range, EX.Category))\n",
        "\n",
        "\n",
        "g.add((EX.containsAllergen, RDF.type, OWL.ObjectProperty))\n",
        "g.add((EX.containsAllergen, RDFS.domain, EX.Product))\n",
        "g.add((EX.containsAllergen, RDFS.range, EX.Allergen))\n",
        "\n",
        "g.add((EX.hasNutrient, RDF.type, OWL.ObjectProperty))\n",
        "g.add((EX.hasNutrient, RDFS.domain, EX.Product))\n",
        "g.add((EX.hasNutrient, RDFS.range, EX.Nutrient))\n",
        "\n",
        "\n",
        "g.add((EX.availableInCountry, RDF.type, OWL.ObjectProperty))\n",
        "g.add((EX.availableInCountry, RDFS.domain, EX.Product))\n",
        "g.add((EX.availableInCountry, RDFS.range, DBO.Country))\n",
        "\n",
        "# Nouvelles propriétés pour les entités supplémentaires\n",
        "g.add((EX.hasCreator, RDF.type, OWL.ObjectProperty))\n",
        "g.add((EX.hasCreator, RDFS.domain, EX.Product))\n",
        "g.add((EX.hasCreator, RDFS.range, EX.Creator))\n",
        "\n",
        "g.add((EX.hasManufacturingPlace, RDF.type, OWL.ObjectProperty))\n",
        "g.add((EX.hasManufacturingPlace, RDFS.domain, EX.Product))\n",
        "g.add((EX.hasManufacturingPlace, RDFS.range, EX.ManufacturingPlace))\n",
        "\n",
        "g.add((EX.hasIngredient, RDF.type, OWL.ObjectProperty))\n",
        "g.add((EX.hasIngredient, RDFS.domain, EX.Product))\n",
        "g.add((EX.hasIngredient, RDFS.range, EX.Ingredient))\n",
        "\n",
        "g.add((EX.hasIngredientsText, RDF.type, OWL.DatatypeProperty))\n",
        "g.add((EX.hasIngredientsText, RDFS.domain, EX.Product))\n",
        "g.add((EX.hasIngredientsText, RDFS.range, XSD.string))\n",
        "\n",
        "# Ajouter les propriétés pour quantity et serving_size\n",
        "g.add((EX.hasQuantity, RDF.type, OWL.DatatypeProperty))\n",
        "g.add((EX.hasQuantity, RDFS.domain, EX.Product))\n",
        "g.add((EX.hasQuantity, RDFS.range, XSD.string))\n",
        "\n",
        "#g.add((EX.hasServingSize, RDF.type, OWL.DatatypeProperty))\n",
        "#g.add((EX.hasServingSize, RDFS.domain, EX.Product))\n",
        "#g.add((EX.hasServingSize, RDFS.range, XSD.string))\n",
        "\n",
        "\n",
        "g.add((EX.hasPurchasePlace, RDF.type, OWL.ObjectProperty))\n",
        "g.add((EX.hasPurchasePlace, RDFS.domain, EX.Product))\n",
        "g.add((EX.hasPurchasePlace, RDFS.range, EX.PurchasePlace))\n",
        "\n",
        "g.add((EX.hasCity, RDF.type, OWL.ObjectProperty))\n",
        "g.add((EX.hasCity, RDFS.domain, EX.Product))\n",
        "g.add((EX.hasCity, RDFS.range, EX.City))\n",
        "\n",
        "\"\"\"g.add((EX.hasEcoscore, RDF.type, OWL.ObjectProperty))\n",
        "g.add((EX.hasEcoscore, RDFS.domain, EX.Product))\n",
        "g.add((EX.hasEcoscore, RDFS.range, EX.Ecoscore))\n",
        "\n",
        "g.add((EX.hasEcoscoreScore, RDF.type, OWL.ObjectProperty))\n",
        "g.add((EX.hasEcoscoreScore, RDFS.domain, EX.Ecoscore))\n",
        "g.add((EX.hasEcoscoreScore, RDFS.range, EX.EcoscoreScore))\n",
        "\n",
        "#g.add((EX.hasEcoscoreGrade, RDF.type, OWL.ObjectProperty))\n",
        "g.add((EX.hasEcoscoreGrade, RDFS.domain, EX.Ecoscore))\n",
        "g.add((EX.hasEcoscoreGrade, RDFS.range, EX.EcoscoreGrade))\"\"\"\n",
        "\n",
        "\n",
        "# Fonction pour encoder les URI de manière sécurisée\n",
        "def encode_uri(uri):\n",
        "    return urllib.parse.quote(uri, safe=\":/#?&=~\")\n",
        "\n",
        "\n",
        "CACHE_FILE = \"country_qcode_cache.json\"\n",
        "\n",
        "def load_cache():\n",
        "    \"\"\"\n",
        "    Charge le cache des résultats depuis un fichier local.\n",
        "    \"\"\"\n",
        "    if os.path.exists(CACHE_FILE):\n",
        "        with open(CACHE_FILE, 'r') as f:\n",
        "            return json.load(f)\n",
        "    return {}\n",
        "\n",
        "def save_cache(cache):\n",
        "    \"\"\"\n",
        "    Sauvegarde le cache des résultats dans un fichier local.\n",
        "    \"\"\"\n",
        "    with open(CACHE_FILE, 'w') as f:\n",
        "        json.dump(cache, f)\n",
        "\n",
        "def normalize_country_name(country_name):\n",
        "    \"\"\"\n",
        "    Nettoie le nom d'un pays pour le rendre standard.\n",
        "    \"\"\"\n",
        "    return country_name.replace(\"en:\", \"\").split(\" - \")[0].strip()\n",
        "\n",
        "def get_country_qcode(country_name, timeout=10):\n",
        "    \"\"\"\n",
        "    Recherche le Q-code d'un pays donné via l'API SPARQL de Wikidata.\n",
        "    Inclut des mécanismes de mise en cache et de gestion des erreurs.\n",
        "\n",
        "    :param country_name: Nom du pays à rechercher\n",
        "    :param timeout: Durée maximale pour la requête en secondes\n",
        "    :return: URI complète du pays avec le Q-code, ou une valeur par défaut si une erreur survient\n",
        "    \"\"\"\n",
        "    # Normaliser et nettoyer le nom du pays\n",
        "    cleaned_country_name = normalize_country_name(country_name)\n",
        "\n",
        "    # Charger le cache\n",
        "    cache = load_cache()\n",
        "\n",
        "    # Vérifier si le résultat est déjà dans le cache\n",
        "    if cleaned_country_name in cache:\n",
        "        print(f\"Using cached result for {cleaned_country_name}\")\n",
        "        return cache[cleaned_country_name]\n",
        "\n",
        "    # Configurer le point d'accès SPARQL\n",
        "    sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
        "\n",
        "    # Construire la requête SPARQL\n",
        "    query = f\"\"\"\n",
        "    SELECT ?country WHERE {{\n",
        "      ?country wdt:P31 wd:Q6256;  # Instance de \"pays\"\n",
        "              rdfs:label \"{cleaned_country_name}\"@en .\n",
        "    }}\n",
        "    LIMIT 1\n",
        "    \"\"\"\n",
        "    sparql.setQuery(query)\n",
        "    sparql.setReturnFormat(JSON)\n",
        "\n",
        "    # Exécuter la requête\n",
        "    try:\n",
        "        sparql.setTimeout(timeout)  # Timeout pour éviter les blocages\n",
        "        results = sparql.query().convert()\n",
        "        for result in results.get(\"results\", {}).get(\"bindings\", []):\n",
        "            qcode_uri = result[\"country\"][\"value\"]\n",
        "            # Ajouter le résultat au cache\n",
        "            cache[cleaned_country_name] = qcode_uri\n",
        "            save_cache(cache)\n",
        "            return qcode_uri\n",
        "    except requests.exceptions.RequestException as req_err:\n",
        "        print(f\"Request error for {country_name}: {req_err}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during SPARQL query for {country_name}: {e}\")\n",
        "\n",
        "    # Retourner une valeur par défaut en cas d'erreur\n",
        "    fallback_uri = \"http://www.wikidata.org/entity/Q30\"  # Par défaut : Q30 (USA)\n",
        "    cache[cleaned_country_name] = fallback_uri\n",
        "    save_cache(cache)\n",
        "    return fallback_uri\n",
        "\n",
        "\n",
        "def get_dbpedia_uri(brand_name):\n",
        "    \"\"\"Renvoie une URI DBpedia pour une marque donnée.\"\"\"\n",
        "\n",
        "    return f\"http://dbpedia.org/resource/{brand_name.replace(' ', '_')}\"\n",
        "\n",
        "\n",
        "def get_wikidata_category_uri(category_name, language=\"en\"):\n",
        "    \"\"\"\n",
        "    Recherche le Q-code d'une catégorie donnée sur Wikidata.\n",
        "\n",
        "    Args:\n",
        "        category_name (str): Le nom de la catégorie à rechercher.\n",
        "        language (str): La langue de recherche (par défaut \"en\" pour l'anglais).\n",
        "\n",
        "    Returns:\n",
        "        str: L'URI Wikidata pour la catégorie, ou None si elle n'est pas trouvée.\n",
        "    \"\"\"\n",
        "    # URL de l'API Wikidata\n",
        "    api_url = \"https://www.wikidata.org/w/api.php\"\n",
        "\n",
        "    # Paramètres pour rechercher une entité correspondant à la catégorie\n",
        "    params = {\n",
        "        \"action\": \"wbsearchentities\",\n",
        "        \"search\": category_name,\n",
        "        \"language\": language,\n",
        "        \"format\": \"json\",\n",
        "        \"type\": \"item\"  # Recherche uniquement les items\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Requête à l'API Wikidata\n",
        "        response = requests.get(api_url, params=params)\n",
        "        response.raise_for_status()  # Lève une exception si le statut HTTP n'est pas 200\n",
        "\n",
        "        # Récupérer les résultats de la recherche\n",
        "        data = response.json()\n",
        "        if data.get(\"search\"):\n",
        "            # Retourner le premier résultat (Q-code)\n",
        "            q_code = data[\"search\"][0][\"id\"]\n",
        "            return f\"http://www.wikidata.org/entity/{q_code}\"\n",
        "        else:\n",
        "\n",
        "            return f\"http://www.wikidata.org/entity/Q11070\"\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Erreur lors de la requête à l'API Wikidata : {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Lire le fichier CSV et ajouter des instances\n",
        "csv_file_path = \"cleaned_file.csv\"  # Remplacez par votre fichier CSV\n",
        "\n",
        "try:\n",
        "    with open(csv_file_path, newline='', encoding='utf-8') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)\n",
        "        print(\" debut lecture csv \\n\")\n",
        "        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "        logging.info(\"Début du traitement du fichier CSV.\")\n",
        "        for row in reader:\n",
        "            if all(col in row for col in important_columns):\n",
        "                product_uri = URIRef(encode_uri(f\"http://produitsalimentaires.fr/{row['code']}\"))\n",
        "\n",
        "                # Ajouter le type de produit\n",
        "                print(\" Ajout produit \\n\")\n",
        "                g.add((product_uri, RDF.type, DBO.Product))\n",
        "                g.add((product_uri, SCHEMA.name, Literal(row[\"product_name\"])))\n",
        "                g.add((product_uri, SCHEMA.url, URIRef(row[\"url\"])))\n",
        "\n",
        "                # Ajouter la marque avec lien vers DBpedia\n",
        "                print(\" Ajout marque produit, association avec dbpedia \\n\")\n",
        "                if row[\"brands\"]:\n",
        "                    brand_name = row[\"brands\"].strip()\n",
        "                    brand_uri = URIRef(encode_uri(f\"http://produitsalimentaires.fr/brand/{row['brands']}\"))\n",
        "                    dbpedia_brand_uri = get_dbpedia_uri(row[\"brands\"])\n",
        "                    g.add((brand_uri, RDF.type, DBO.Brand))\n",
        "                    g.add((brand_uri, FOAF.name, Literal(row[\"brands\"])))\n",
        "                    if dbpedia_brand_uri:\n",
        "\n",
        "                        g.add((brand_uri, DBO.wikiPage, URIRef(encode_uri(dbpedia_brand_uri))))\n",
        "\n",
        "                    g.add((product_uri, EX.hasBrand, brand_uri))\n",
        "\n",
        "\n",
        "                # Ajouter des catégories basées sur le champ 'categories' avec lien vers Wikidata\n",
        "\n",
        "                if row[\"categories\"]:\n",
        "                    categories = row[\"categories\"].split(',')  # Séparer les catégories par des virgules\n",
        "                    for category in categories:\n",
        "                        category = category.strip()\n",
        "                        if category:  # Vérifier que la catégorie n'est pas vide\n",
        "                            category_uri = URIRef(encode_uri(f\"http://produitsalimentaires.fr/category/{category}\"))\n",
        "                            wikidata_category_uri = get_wikidata_category_uri(category)\n",
        "                            g.add((category_uri, RDF.type, EX.Category))\n",
        "                            g.add((category_uri, SCHEMA.name, Literal(category)))\n",
        "                            g.add((category_uri, DBO.wikiPage, URIRef(encode_uri(wikidata_category_uri))))\n",
        "\n",
        "                            g.add((product_uri, EX.hasCategory, category_uri))\n",
        "\n",
        "\n",
        "                # Ajouter des relations d'origine basées sur le champ 'countries' avec lien vers Wikidata\n",
        "\n",
        "                if row[\"countries\"]:\n",
        "                    countries = row[\"countries\"].split(',')  # Séparer les pays par des virgules\n",
        "                    for country in countries:\n",
        "                        country = country.strip()\n",
        "                        if country:  # Vérifier que le pays n'est pas vide\n",
        "                            country_uri = URIRef(encode_uri(f\"http://produitsalimentaires.fr/country/{country}\"))\n",
        "\n",
        "                            normalized_country = normalize_country_name(country)\n",
        "\n",
        "                            qcode_uri = get_country_qcode(normalized_country)\n",
        "\n",
        "                            #wikidata_country_uri = get_wikidata_uri(country)\n",
        "\n",
        "                            g.add((country_uri, RDF.type, DBO.Country))\n",
        "\n",
        "                            g.add((country_uri, FOAF.name, Literal(normalized_country)))\n",
        "\n",
        "\n",
        "                            g.add((country_uri, DBO.wikiPage, URIRef(encode_uri(qcode_uri))))\n",
        "\n",
        "                            g.add((product_uri, EX.availableInCountry, country_uri))\n",
        "\n",
        "                if row[\"allergens\"]:\n",
        "                    allergens = row[\"allergens\"].split(',')  # Séparer les valeurs par des virgules\n",
        "                    for allergen in allergens:\n",
        "                        allergen = allergen.strip()\n",
        "                        if allergen:  # Vérifier que l'origine n'est pas vide\n",
        "                            # Créer une instance de l'origine\n",
        "                            allergen_uri = URIRef(encode_uri(f\"http://produitsalimentaires.fr/allergens/{allergen}\"))\n",
        "\n",
        "                            g.add((product_uri, EX.containsAllergen, allergen_uri))\n",
        "                            g.add((allergen_uri, RDF.type, EX.Allergen))\n",
        "                            g.add((allergen_uri, SCHEMA.name, Literal(allergen)))\n",
        "\n",
        "\n",
        "\n",
        "                # Ajouter des nutriments\n",
        "                nutrients = [\n",
        "                    (\"energy_100g\", \"Energy\"),\n",
        "                    (\"fat_100g\", \"Fat\"),\n",
        "                    (\"saturated-fat_100g\", \"Saturated Fat\"),\n",
        "                    (\"carbohydrates_100g\", \"Carbohydrates\"),\n",
        "                    (\"sugars_100g\", \"Sugars\"),\n",
        "                    (\"fiber_100g\", \"Fiber\"),\n",
        "                    (\"proteins_100g\", \"Proteins\"),\n",
        "                    (\"salt_100g\", \"Salt\"),\n",
        "                    (\"carbon-footprint_100g\", \"carbon footprint\")\n",
        "                ]\n",
        "\n",
        "                for nutrient, label in nutrients:\n",
        "                    if row[nutrient]:\n",
        "                        nutrient_uri = URIRef(encode_uri(f\"http://produitsalimentaires.fr/nutrient/{label}\"))\n",
        "                        g.add((product_uri, EX.hasNutrient, nutrient_uri))\n",
        "                        g.add((nutrient_uri, RDF.type, EX.Nutrient))\n",
        "                        g.add((nutrient_uri, SCHEMA.name, Literal(label)))\n",
        "                        g.add((nutrient_uri, SCHEMA.value, Literal(row[nutrient], datatype=XSD.float)))\n",
        "\n",
        "\n",
        "                # Ajouter des ingrédients\n",
        "\n",
        "                if row[\"ingredients_text\"]:\n",
        "                    g.add((product_uri, EX.hasIngredientsText, Literal(row[\"ingredients_text\"])))\n",
        "\n",
        "\n",
        "                # Ajouter les créateurs\n",
        "\n",
        "                if row[\"creator\"]:\n",
        "                    creator_uri = URIRef(encode_uri(f\"http://produitsalimentaires.fr/creator/{row['creator']}\"))\n",
        "                    g.add((product_uri, EX.hasCreator, creator_uri))\n",
        "                    g.add((creator_uri, RDF.type, EX.Creator))\n",
        "                    g.add((creator_uri, SCHEMA.name, Literal(row[\"creator\"])))\n",
        "\n",
        "\n",
        "                # Ajouter les lieux de fabrication\n",
        "\n",
        "                if row[\"manufacturing_places\"]:\n",
        "                    manufacturing = row[\"manufacturing_places\"].split(',')  # Séparer les catégories par des virgules\n",
        "                    for manu in manufacturing:\n",
        "                        cmanu = manu.strip()\n",
        "                        if category:  # Vérifier que la catégorie n'est pas vide\n",
        "                            manufacturing_place_uri = URIRef(encode_uri(f\"http://produitsalimentaires.fr/manufacturing_place/{manu}\"))\n",
        "                            g.add((product_uri, EX.hasManufacturingPlace, manufacturing_place_uri))\n",
        "                            g.add((manufacturing_place_uri, RDF.type, EX.ManufacturingPlace))\n",
        "                            g.add((manufacturing_place_uri, SCHEMA.name, Literal(manu)))\n",
        "\n",
        "                if row[\"purchase_places\"]:\n",
        "                    purchase_place_uri = URIRef(encode_uri(f\"http://produitsalimentaires.fr/purchase_place/{row['purchase_places']}\"))\n",
        "                    g.add((product_uri, EX.hasPurchasePlace, purchase_place_uri))\n",
        "                    g.add((purchase_place_uri, RDF.type, EX.PurchasePlace))\n",
        "                    g.add((purchase_place_uri, SCHEMA.name, Literal(row[\"purchase_places\"])))\n",
        "\n",
        "                # Ajouter les villes\n",
        "\n",
        "                if row[\"cities\"]:\n",
        "                    city_uri = URIRef(encode_uri(f\"http://produitsalimentaires.fr/city/{row['cities']}\"))\n",
        "                    g.add((product_uri, EX.hasCity, city_uri))\n",
        "                    g.add((city_uri, RDF.type, EX.City))\n",
        "                    g.add((city_uri, SCHEMA.name, Literal(row[\"cities\"])))\n",
        "\n",
        "\n",
        "                  # Ajouter la quantité (quantity)\n",
        "                print(\" Ajout quantite produit, association avec dbpedia \\n\")\n",
        "                if row[\"quantity\"]:\n",
        "                    g.add((product_uri, EX.hasQuantity, Literal(row[\"quantity\"])))\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Erreur lors du chargement du fichier CSV : {e}\")\n",
        "    exit()\n",
        "\n",
        "# Sauvegarder le graphe OWL\n",
        "g.serialize(destination=rdf_file_path, format=\"xml\")\n",
        "print(f\"Ontologie OWL générée et sauvegardée dans : {rdf_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qB0408bT2PRo",
        "outputId": "920c72d9-0c32-469e-c89a-8626643864e4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " debut lecture csv \n",
            "\n",
            "Ontologie OWL générée et sauvegardée dans : output_ontology2.rdf\n"
          ]
        }
      ]
    }
  ]
}